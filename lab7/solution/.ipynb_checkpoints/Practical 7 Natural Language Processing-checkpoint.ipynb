{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ebc1e1",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accaacba",
   "metadata": {},
   "source": [
    "# Classifying Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6142b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham number is 4824\n",
      "spam number is 747\n",
      "TP=47 FP=2287 TN=155 FN=297\n",
      "precision=0.020137103684661525 recall=0.13662790697674418 f1=0.0351008215085885\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn.model_selection as modsel\n",
    "import sklearn.naive_bayes as nb\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv( '../data/SMSSpamCollection.csv', na_filter=False, encoding='latin-1' )\n",
    "except Exception as x:\n",
    "    print('error> reading data file' + str( x ))\n",
    "    sys.exit()\n",
    "M = len( df.values )\n",
    "N = len( df.columns )\n",
    "if ( DEBUGGING ):\n",
    "    print('number of instances = ' + str( M ))\n",
    "    print('number of columns = ' + str( N ))\n",
    "\n",
    "ham = 0\n",
    "spam = 0\n",
    "for i in range (M):\n",
    "    if(df.values[i][0] =='ham'):\n",
    "        ham += 1\n",
    "    elif(df.values[i][0] == 'spam'):\n",
    "        spam += 1;\n",
    "print(\"ham number is \" + str(ham))\n",
    "print(\"spam number is \" + str(spam))\n",
    "\n",
    "msgs   = []\n",
    "y_raw = []\n",
    "for rec in df.values:\n",
    "    try:\n",
    "        label = rec[0].strip()\n",
    "        msg   = rec[1].strip()\n",
    "        words = nltk.word_tokenize(msg)\n",
    "        msgs.append(msg)\n",
    "        y_raw.append(label)\n",
    "    except Exception as x:\n",
    "        print('error> parsing raw data file: ' + str( x ))\n",
    "        \n",
    "if ( DEBUGGING ):\n",
    "    print('msgs shape = ', np.shape( msgs ), 'y_raw shape = ', np.shape( y_raw ))\n",
    "    \n",
    "msgs_train,msgs_test,y_train,y_test = modsel.train_test_split(msgs,y_raw,test_size = 0.5)\n",
    "if ( DEBUGGING ):\n",
    "    print('size of data sets:')\n",
    "    print('msgs:   training={} test={}'.format( len( msgs_train ), len( msgs_test )))\n",
    "    print('labels: training={} test={}'.format( len( y_train ), len( y_test )))\n",
    "\n",
    "#--STEP 3: CREATE BALANCED TRAINING SET\n",
    "ham = []\n",
    "spam = []\n",
    "numerr = 0\n",
    "for (msg, label) in zip(msgs_train,y_raw):\n",
    "    if(label == 'ham'):\n",
    "        ham.append(msg)\n",
    "    elif(label == 'spam'):\n",
    "        spam.append(msg)\n",
    "    else:\n",
    "        numerr += 1\n",
    "if ( DEBUGGING):\n",
    "    print('number of: ham={} spam={} errors={} total={}'.format( len( ham ), len( spam ), numerr, ( len( ham ) + len( spam ) + numerr )))\n",
    "    \n",
    "while(len(spam) < len(ham)):\n",
    "    i = random.randint(0,len(spam)-1)\n",
    "    spam.append(spam[i])\n",
    "    \n",
    "if ( DEBUGGING ):\n",
    "    print('balanced! number of: ham={} spam={}'.format( len(ham), len(spam) ))\n",
    "    \n",
    "msgs_bal_train = []\n",
    "y_bal_train = []\n",
    "for i in range(len(spam)):\n",
    "    msgs_bal_train.append(spam[i])\n",
    "    y_bal_train.append(\"spam\")\n",
    "for i in range(len(ham)):\n",
    "    msgs_bal_train.append(ham[i])\n",
    "    y_bal_train.append(\"ham\")\n",
    "    \n",
    "if ( DEBUGGING ):\n",
    "    print('size of balanced training: msgs={} labels={}'.format( len( msgs_bal_train ), len( y_bal_train )))\n",
    "    \n",
    "#STEP 4 CHARACTERISE THE TRAINING SET\n",
    "# attributes:\n",
    "# num_words\n",
    "# msg_len\n",
    "# num_digits\n",
    "# num_punct\n",
    "# num_upper\n",
    "num_attributes = 5\n",
    "X = []\n",
    "\n",
    "for msg in (msgs_bal_train):\n",
    "    num_digits = 0\n",
    "    num_punct  = 0\n",
    "    num_upper  = 0\n",
    "    \n",
    "    for ch in msg:\n",
    "        if(ch.isdigit()):\n",
    "            num_digits += 1\n",
    "        elif(ch in string.punctuation):\n",
    "            num_punct += 1\n",
    "        elif(ch.isupper()):\n",
    "            num_upper += 1\n",
    "    msg_len = len(msg)\n",
    "    words = nltk.word_tokenize(msg)\n",
    "    num_words = len(words)\n",
    "    X.append((num_words, msg_len, num_digits, num_punct, num_upper))\n",
    "    \n",
    "X = np.array(X)\n",
    "y_bal_train  = np.array(y_bal_train)\n",
    "\n",
    "#--STEP 5: TRAIN CLASSIFIER\n",
    "\n",
    "clf = nb.MultinomialNB()\n",
    "clf.fit(X, y_bal_train)\n",
    "\n",
    "#test classfier\n",
    "num_TP = 0\n",
    "num_TN = 0\n",
    "num_FP = 0\n",
    "num_FN = 0\n",
    "\n",
    "# attributes:\n",
    "# num_words\n",
    "# msg_len\n",
    "# num_digits\n",
    "# num_punct\n",
    "# num_upper\n",
    "\n",
    "A = []\n",
    "for (msg,label) in zip(msgs_test,y_test):\n",
    "#     A = []\n",
    "    words = nltk.word_tokenize(msg)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    num_digits = 0\n",
    "    num_punct  = 0\n",
    "    num_upper  = 0\n",
    "    msg_len = len(msg)\n",
    "    for ch in msg:\n",
    "        if(ch.isdigit()):\n",
    "            num_digits +=1\n",
    "        if(ch.isupper()):\n",
    "            num_upper += 1\n",
    "        if(ch in string.punctuation):\n",
    "            num_punct += 1\n",
    "    A.append((num_words, msg_len, num_digits, num_punct, num_upper))\n",
    "\n",
    "\n",
    "#     pred_label = clf.predict(A)\n",
    "#     if ( label == 'spam' ):\n",
    "#         if ( pred_label == 'spam' ):\n",
    "#             num_TP += 1\n",
    "#         elif ( pred_label == 'ham' ):\n",
    "#             num_FN += 1\n",
    "#         else:\n",
    "#             num_err += 1\n",
    "#     elif ( label == 'ham' ):\n",
    "#         if ( pred_label == 'spam' ):\n",
    "#             num_FP += 1\n",
    "#         elif ( pred_label == 'ham' ):\n",
    "#             num_TN += 1\n",
    "#         else:\n",
    "#             num_err += 1\n",
    "#     else:\n",
    "#         num_err += 1\n",
    "\n",
    "y_hat = clf.predict(A)\n",
    "for i in range (len(y_hat)):\n",
    "    if(y_test[i] == 'spam'):\n",
    "        if(y_hat[i] == 'spam'):\n",
    "            num_TP += 1\n",
    "        else:\n",
    "            num_FN += 1\n",
    "    else:\n",
    "        if ( y_hat[i] == 'spam' ):\n",
    "            num_FP += 1\n",
    "        else:\n",
    "            num_TN += 1\n",
    "            \n",
    "print('TP={} FP={} TN={} FN={}'.format( num_TP, num_FP, num_TN, num_FN ))\n",
    "precision = num_TP / float( num_TP + num_FP )\n",
    "recall = num_TP / float( num_TP + num_FN )\n",
    "f1 = 2 * precision * recall / ( precision + recall )\n",
    "print('precision={} recall={} f1={}'.format( precision, recall, f1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af5de1",
   "metadata": {},
   "source": [
    "# Clustering Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7ef95",
   "metadata": {},
   "source": [
    "## setp 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc00dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "1 = the, 20\n",
      "2 = a, 13\n",
      "3 = and, 8\n",
      "4 = they, 7\n",
      "5 = of, 7\n",
      "6 = you, 7\n",
      "7 = to, 6\n",
      "8 = pussy, 5\n",
      "9 = are, 5\n",
      "10 = in, 4\n",
      "10 most frequent words, after removing stopwords:\n",
      "1 = pussy, 5\n",
      "2 = ring, 4\n",
      "3 = nose, 4\n",
      "4 = moon, 4\n",
      "5 = owl, 3\n",
      "6 = beautiful, 3\n",
      "7 = married, 2\n",
      "8 = day, 2\n",
      "9 = end, 2\n",
      "10 = hand, 2\n"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "import requests\n",
    "\n",
    "TOP_MOST = 10\n",
    "\n",
    "with open(\"../data/el-owl-cat.txt\") as f:\n",
    "    raw_verse = f.read()\n",
    "f.close()\n",
    "\n",
    "verse = textblob.TextBlob(raw_verse)\n",
    "# print(verse.words)\n",
    "print(len(verse.words))\n",
    "# print(verse.word_counts)\n",
    "\n",
    "# top most frequent\n",
    "sorted_words = sorted(verse.word_counts,key=verse.word_counts.__getitem__, reverse=True)\n",
    "for (i ,w ) in zip(range(TOP_MOST),sorted_words):\n",
    "    print('{} = {}, {}'.format(( i+1) , w, verse.word_counts[w] ))\n",
    "    \n",
    "#get the stopwords\n",
    "stopwords = requests.get(\"https://raw.githubusercontent.com/fozziethebeat/S-Space/master/data/english-stop-words-large.txt\" ).content.decode(\"UTF-8\").split(\"\\n\")\n",
    "\n",
    "words = {}\n",
    "for key in sorted_words:\n",
    "    if (len(words) >10):\n",
    "        break;\n",
    "    if(key not in stopwords):\n",
    "        words[key] = verse.word_counts[key]\n",
    "        \n",
    "print('{} most frequent words, after removing stopwords:'.format( TOP_MOST ))\n",
    "for ( i, w ) in zip( range( TOP_MOST ), words ):\n",
    "    print('{} = {}, {}'.format(( 1+i) , w, verse.word_counts[w] ))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b70fd",
   "metadata": {},
   "source": [
    "## step 5-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb4a6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance from 0 to 1 = 4.0\n",
      "Euclidean distance from 0 to 2 = 4.47213595499958\n",
      "Euclidean distance from 0 to 3 = 4.47213595499958\n",
      "Euclidean distance from 0 to 4 = 4.47213595499958\n",
      "Euclidean distance from 1 to 2 = 4.47213595499958\n",
      "Euclidean distance from 1 to 3 = 4.47213595499958\n",
      "Euclidean distance from 1 to 4 = 4.47213595499958\n",
      "Euclidean distance from 2 to 3 = 4.242640687119285\n",
      "Euclidean distance from 2 to 4 = 4.47213595499958\n",
      "Euclidean distance from 3 to 4 = 4.0\n",
      "0,1, dist is 4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "TOP_MOST = 10\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "DATA_FILES = [ 'a-3kittens.txt', 'bp-tom-kitten.txt', 'el-owl-cat.txt', 'rc-cat-fiddle.txt', 'rk-cat.txt' ]\n",
    "\n",
    "stop_words = requests.get(\"https://raw.githubusercontent.com/fozziethebeat/S-Space/master/da\\\n",
    "ta/english-stop-words-large.txt\").content.decode(\"UTF-8\").split(\"\\n\")\n",
    "\n",
    "freq_words = [{} for i in range (len(DATA_FILES))]\n",
    "for i in range(len(DATA_FILES)):\n",
    "    with open(DATA_DIR+DATA_FILES[i]) as f:\n",
    "        raw_verse = f.read()\n",
    "    f.close()\n",
    "    verse = textblob.TextBlob(raw_verse)\n",
    "    sorted_words = sorted(verse.word_counts,key = verse.word_counts.__getitem__,reverse= True)\n",
    "    \n",
    "    for key in sorted_words:\n",
    "        if(len(freq_words[i])==10):\n",
    "            break;\n",
    "        if(key in stop_words):\n",
    "            continue;\n",
    "        freq_words[i][key] = verse.word_counts[key]\n",
    "\n",
    "        \n",
    "#step 6 Boolean term-document\n",
    "terms = []\n",
    "for i in range (len(DATA_FILES)):\n",
    "    for key in freq_words[i]:\n",
    "        if(key not in terms):\n",
    "            terms.append(key)\n",
    "            \n",
    "#empty matrix\n",
    "termdoc = [ [0 for key in range(len(terms))] for j in range (len(DATA_FILES))]\n",
    "\n",
    "#build the matrix\n",
    "for i in range (len(DATA_FILES)):\n",
    "    for j in range(len(terms)):\n",
    "        if(terms[j] in freq_words[i]):\n",
    "            termdoc[i][j] = 1\n",
    "\n",
    "#step 7 Euclidean distance\n",
    "def euc_dist(j0,j1,num_t,termdoc):\n",
    "    dist = 0.0\n",
    "    for i in range (num_t):\n",
    "        dist += np.square(termdoc[j0][i] - termdoc[j1][i])\n",
    "    dist = np.sqrt(dist)\n",
    "    return dist\n",
    "\n",
    "min_d = euc_dist(0,1,len(terms),termdoc)\n",
    "min_0 = 0\n",
    "min_1 = 1\n",
    "for j0 in range(len(DATA_FILES)):\n",
    "    for j1 in range(j0+1,len(DATA_FILES)):\n",
    "        dist = euc_dist(j0,j1,len(terms),termdoc)\n",
    "        print('Euclidean distance from {} to {} = {}'.format( j0, j1, dist ))\n",
    "        if(dist< min_d):\n",
    "            min_d = dist\n",
    "            min_0 = j0\n",
    "            min_1 = j1\n",
    "print('{},{}, dist is {}'.format(min_0,min_1,min_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838cc79",
   "metadata": {},
   "source": [
    "## step 8-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8828ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance from 0 to 1 = 74.15524256584965\n",
      "Euclidean distance from 0 to 2 = 73.58668357794092\n",
      "Euclidean distance from 0 to 3 = 72.94518489934754\n",
      "Euclidean distance from 0 to 4 = 168.5734261382855\n",
      "Euclidean distance from 1 to 2 = 24.413111231467404\n",
      "Euclidean distance from 1 to 3 = 22.40535650240808\n",
      "Euclidean distance from 1 to 4 = 153.6164053739053\n",
      "Euclidean distance from 2 to 3 = 10.583005244258363\n",
      "Euclidean distance from 2 to 4 = 152.3679756379273\n",
      "Euclidean distance from 3 to 4 = 151.36710342739602\n",
      "2,3, dist is 10.583005244258363\n",
      "cos similarity  from 0 to 1 = 45.224652749107065\n",
      "cos similarity  from 0 to 2 = 0.0\n",
      "cos similarity  from 0 to 3 = 0.0\n",
      "cos similarity  from 0 to 4 = 0.0\n",
      "cos similarity  from 1 to 2 = 0.0\n",
      "cos similarity  from 1 to 3 = 0.0\n",
      "cos similarity  from 1 to 4 = 0.0\n",
      "cos similarity  from 2 to 3 = 1.3942471924464683\n",
      "cos similarity  from 2 to 4 = 0.0\n",
      "cos similarity  from 3 to 4 = 4426.986516286741\n",
      "3,4, cos similarity is 4426.986516286741\n"
     ]
    }
   ],
   "source": [
    "import textblob\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "TOP_MOST = 10\n",
    "DATA_DIR = '../data/'\n",
    "DATA_FILES = [ 'a-3kittens.txt', 'bp-tom-kitten.txt', 'el-owl-cat.txt', 'rc-cat-fiddle.txt', 'rk-cat.txt' ]\n",
    "\n",
    "#get the stop words\n",
    "stopwords = requests.get(\"https://raw.githubusercontent.com/fozziethe\\\n",
    "beat/S-Space/master/data/english-stop-words-large.txt\").content.decode(\"UTF-8\").split(\"\\n\")\n",
    "\n",
    "#get all the frequent words\n",
    "freq_words = [{} for i in range(len(DATA_FILES))]\n",
    "\n",
    "for i in range (len(DATA_FILES)):\n",
    "    #open the file\n",
    "    with open(DATA_DIR+DATA_FILES[i]) as f:\n",
    "        raw_verse = f.read()\n",
    "        verse = textblob.TextBlob(raw_verse)\n",
    "    f.close()\n",
    "    #get the frequent word in file\n",
    "    sorted_words = sorted(verse.word_counts,key= verse.word_counts.__getitem__, reverse=True)\n",
    "    for key in sorted_words:\n",
    "        if(len(freq_words[i]) == 10):\n",
    "            break\n",
    "        if(key in stopwords):\n",
    "            continue;\n",
    "        if(key not in freq_words[i]):\n",
    "            freq_words[i][key] = verse.word_counts[key]\n",
    "\n",
    "#build all the terms\n",
    "terms = []\n",
    "for i in range(len(DATA_FILES)):\n",
    "    for key in freq_words[i]:\n",
    "        if(key not in terms):\n",
    "            terms.append(key)\n",
    "\n",
    "# step 8 build the frequency term-document matrix\n",
    "termdoc = [[0 for i in range(len(terms))]  for i in range(len(DATA_FILES))]\n",
    "for i in range(len(DATA_FILES)):\n",
    "    for j in range(len(terms)):\n",
    "        if(terms[j] in freq_words[i]):\n",
    "            termdoc[i][j] = freq_words[i][terms[j]]\n",
    "\n",
    "#step 9 Euclidean distance \n",
    "def euc_dist(j0,j1,termdoc):\n",
    "    length = len(termdoc[0])\n",
    "    dist = 0.0\n",
    "    for i in range (length):\n",
    "        dist += np.square(termdoc[j0][i] - termdoc[j1][i])\n",
    "    dist = np.sqrt(dist)\n",
    "    return dist\n",
    "\n",
    "min_d = euc_dist(0,1,termdoc)\n",
    "min_0 = 0\n",
    "min_1 = 1 \n",
    "for i in range(len(DATA_FILES)):\n",
    "    for j in range(i+1,len(DATA_FILES)):\n",
    "        dist = euc_dist(i,j,termdoc)\n",
    "        print('Euclidean distance from {} to {} = {}'.format( i, j, dist ))\n",
    "        if(dist< min_d):\n",
    "            min_d = dist\n",
    "            min_0 = i\n",
    "            min_1 = j\n",
    "        \n",
    "print('{},{}, dist is {}'.format(min_0,min_1,min_d))\n",
    "\n",
    "\n",
    "# step 10 Cosine similarity  with frequency term-document matrix\n",
    "def cos_sim( j0, j1, termdoc ):\n",
    "    num_t = len(termdoc[0])\n",
    "    sim_top = 0\n",
    "    sim_bottom_0 = 0\n",
    "    sim_bottom_1 = 0\n",
    "    for t in range (num_t):\n",
    "        sim_top += termdoc[j0][t] * termdoc[j1][t]\n",
    "        sim_bottom_0 += np.square( termdoc[j0][t] )\n",
    "        sim_bottom_1 += np.square( termdoc[j1][t] )\n",
    "    sim = sim_top / ( np.sqrt( sim_bottom_0 )) * ( np.sqrt( sim_bottom_1 ))\n",
    "    return sim\n",
    "\n",
    "max_d  = cos_sim( 0, 1,  termdoc )\n",
    "max_j0 = 0\n",
    "max_j1 = 1\n",
    "\n",
    "for i in range(len(DATA_FILES)):\n",
    "    for j in range(i+1,len(DATA_FILES)):\n",
    "        dist = cos_sim(i,j,termdoc)\n",
    "        print('cos similarity  from {} to {} = {}'.format( i, j, dist ))\n",
    "        if(dist> max_d):\n",
    "            max_d = dist\n",
    "            max_j0 = i\n",
    "            max_j1 = j\n",
    "        \n",
    "print('{},{}, cos similarity is {}'.format(max_j0,max_j1,max_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3859dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching list of stopwords...\n",
      "number of stopwords = 593\n",
      "file= a-3kittens.txt\n",
      "0 miew 62\n",
      "1 purr 32\n",
      "2 mittens 14\n",
      "3 kittens 13\n",
      "4 began 4\n",
      "5 mammy 4\n",
      "6 dear 4\n",
      "7 lost 3\n",
      "8 cry 3\n",
      "9 pie 3\n",
      "file= bp-tom-kitten.txt\n",
      "0 tom 10\n",
      "1 pat 10\n",
      "2 moppet 9\n",
      "3 mittens 6\n",
      "4 kitten 6\n",
      "5 puddle-duck 6\n",
      "6 kittens 5\n",
      "7 tabitha 5\n",
      "8 wall 5\n",
      "9 pit 5\n",
      "file= el-owl-cat.txt\n",
      "0 pussy 5\n",
      "1 ring 4\n",
      "2 nose 4\n",
      "3 moon 4\n",
      "4 owl 3\n",
      "5 beautiful 3\n",
      "6 married 2\n",
      "7 day 2\n",
      "8 end 2\n",
      "9 hand 2\n",
      "file= rc-cat-fiddle.txt\n",
      "0 diddle 2\n",
      "1 hey 1\n",
      "2 cat 1\n",
      "3 fiddle 1\n",
      "4 cow 1\n",
      "5 jumped 1\n",
      "6 moon 1\n",
      "7 dog 1\n",
      "8 laughed 1\n",
      "9 fun 1\n",
      "file= rk-cat.txt\n",
      "0 wild 104\n",
      "1 cat 75\n",
      "2 woman 46\n",
      "3 cave 40\n",
      "4 dog 30\n",
      "5 enemy 25\n",
      "6 woods 21\n",
      "7 man 20\n",
      "8 fire 19\n",
      "9 horse 15\n",
      "cos similarity from 0 to 1 = 45.224652749107065\n",
      "cos similarity from 0 to 2 = 0.0\n",
      "cos similarity from 0 to 3 = 0.0\n",
      "cos similarity from 0 to 4 = 0.0\n",
      "cos similarity from 1 to 2 = 0.0\n",
      "cos similarity from 1 to 3 = 0.0\n",
      "cos similarity from 1 to 4 = 0.0\n",
      "cos similarity from 2 to 3 = 1.3942471924464683\n",
      "cos similarity from 2 to 4 = 0.0\n",
      "cos similarity from 3 to 4 = 4426.986516286741\n",
      "closest two verses by Cosine similarity are: rc-cat-fiddle.txt (3) and rk-cat.txt (4)\n",
      "vectors=\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 75, 0, 0, 0, 30, 0, 0, 104, 46, 40, 25, 21, 20, 19, 15]\n"
     ]
    }
   ],
   "source": [
    "#--\n",
    "# p5.py\n",
    "# # finds two closest documents using frequency term-document matrix and cosine similarity\n",
    "# @author: letsios, sklar\n",
    "# @created: 28 Jan 2021\n",
    "#--\n",
    "\n",
    "import textblob\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "TOP_MOST = 10\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "DATA_FILES = [ 'a-3kittens.txt', 'bp-tom-kitten.txt', 'el-owl-cat.txt', 'rc-cat-fiddle.txt', 'rk-cat.txt' ]\n",
    "\n",
    "\n",
    "#--\n",
    "# cos_sim()\n",
    "# computes and returns cosine similarity between two vectors j0 and j1 in termdoc matrix\n",
    "#--\n",
    "def cos_sim( j0, j1, num_t, termdoc ):\n",
    "    #print 'j0: ',termdoc[j0][:]\n",
    "    #print 'j1: ',termdoc[j1][:]\n",
    "    sim_top = 0\n",
    "    sim_bottom_0 = 0\n",
    "    sim_bottom_1 = 0\n",
    "    for t in range( num_t ):\n",
    "        sim_top += termdoc[j0][t] * termdoc[j1][t]\n",
    "        sim_bottom_0 += np.square( termdoc[j0][t] )\n",
    "        sim_bottom_1 += np.square( termdoc[j1][t] )\n",
    "    sim = sim_top / ( np.sqrt( sim_bottom_0 )) * ( np.sqrt( sim_bottom_1 ))\n",
    "    return( sim )\n",
    "\n",
    "\n",
    "#-----\n",
    "# MAIN\n",
    "#-----\n",
    "\n",
    "# initalise list of dictionaries of most frequent words in each verse\n",
    "freq_words = [ dict() for j in range( len( DATA_FILES )) ]\n",
    "\n",
    "# get list of \"stopwords\"\n",
    "print('fetching list of stopwords...')\n",
    "stopwords = requests.get( \"https://raw.githubusercontent.com/fozziethebeat/S-Space/master/data/english-stop-words-large.txt\" ).content.decode('utf-8').split( \"\\n\" )\n",
    "print('number of stopwords = ' + str( len( stopwords )))\n",
    "if ( DEBUGGING ):\n",
    "    print('stopwords=', stopwords)\n",
    "\n",
    "#-loop to read in the verses\n",
    "for ( j, myfile ) in zip( range( len( DATA_FILES )), DATA_FILES ):\n",
    "    with open( DATA_DIR+myfile ) as f:\n",
    "        raw_verse = f.read()\n",
    "    f.close()\n",
    "    if ( DEBUGGING ):\n",
    "        print('raw_verse=', raw_verse)\n",
    "    print('file=', myfile)\n",
    "\n",
    "    # initialise a TextBlob object using the verse\n",
    "    # (this will decode any UTF-8 characters in the file)\n",
    "    verse = textblob.TextBlob( raw_verse) \n",
    "    if ( DEBUGGING ):\n",
    "        print('verse=', verse)\n",
    "\n",
    "    # create a dictionary of words for this verse, removing the stopwords\n",
    "    words = {}\n",
    "    for w in verse.word_counts:\n",
    "        if ( w not in stopwords ):\n",
    "            words[w] = verse.word_counts[w]\n",
    "    if ( DEBUGGING ):\n",
    "        print(words)\n",
    "\n",
    "    # sort the words in order to find the TOP_MOST most frequent\n",
    "    sorted_words = sorted( words, key=words.__getitem__, reverse=True )\n",
    "    for ( i, w ) in zip( range( TOP_MOST ), sorted_words ):\n",
    "        freq_words[j][w] = verse.word_counts[w]\n",
    "        print(i, w, verse.word_counts[w])\n",
    "\n",
    "# now, the freq_words list contains a dictionary of the TOP_MOST most\n",
    "# frequent words in each data file, and the word frequencies\n",
    "if ( DEBUGGING ):\n",
    "    print(freq_words)\n",
    "\n",
    "# let's use this to create a term-document matrix\n",
    "# start by getting a unique list of terms\n",
    "terms = []\n",
    "for j in range( len( DATA_FILES )):\n",
    "    for w in freq_words[j]:\n",
    "        if ( w not in terms ):\n",
    "            terms.append( w )\n",
    "if ( DEBUGGING ):\n",
    "    print('terms=', terms)\n",
    "\n",
    "# now we can use this to create a binary term-document matrix\n",
    "termdoc = [[0 for t in range( len( terms ))] for j in range( len( DATA_FILES ))]\n",
    "for j in range( len( DATA_FILES )):\n",
    "    for t in range( len( terms )):\n",
    "        if ( terms[t] in freq_words[j] ):\n",
    "            termdoc[j][t] = freq_words[j][ terms[t] ]\n",
    "\n",
    "# print term-document matrix\n",
    "if ( DEBUGGING ):\n",
    "    for t in range( len( terms )):\n",
    "        print( terms[t], end='' )\n",
    "    print()\n",
    "    for j in range( len( DATA_FILES )):\n",
    "        print( DATA_FILES[j], end='' )\n",
    "        for t in range( len( terms )):\n",
    "            print( termdoc[j][t], end='' )\n",
    "        print()\n",
    "\n",
    "# compute pairwise cosine similarity between document vectors\n",
    "max_d  = cos_sim( 0, 1, len( terms ), termdoc )\n",
    "max_j0 = 0\n",
    "max_j1 = 1\n",
    "for j0 in range( len( DATA_FILES )):\n",
    "    for j1 in range( j0+1, len( DATA_FILES )):\n",
    "        d = cos_sim( j0, j1, len( terms ), termdoc )\n",
    "        print('cos similarity from {} to {} = {}'.format( j0, j1, d ))\n",
    "        if ( d > max_d ):\n",
    "            max_d = d\n",
    "            max_j0 = j0\n",
    "            max_j1 = j1\n",
    "print('closest two verses by Cosine similarity are: {} ({}) and {} ({})'.format( DATA_FILES[max_j0], max_j0, DATA_FILES[max_j1], max_j1 ))\n",
    "print('vectors=')\n",
    "print(termdoc[max_j0][:])\n",
    "print(termdoc[max_j1][:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

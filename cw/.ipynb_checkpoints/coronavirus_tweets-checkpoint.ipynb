{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a2e298f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as model_select\n",
    "import sklearn.tree as tree\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Part 3: Mining text data.\n",
    "\n",
    "# Return a pandas dataframe containing the data set.\n",
    "# Specify a 'latin-1' encoding when reading the data.\n",
    "# data_file will be populated with the string 'wholesale_customers.csv'.\n",
    "def read_csv_3(data_file):\n",
    "    DATA_DIR  = './data/'\n",
    "    DATA_FILE = data_file\n",
    "    try:\n",
    "        rawdata = pd.read_csv(DATA_DIR + DATA_FILE, encoding = 'latin-1')\n",
    "    except IOError as iox:\n",
    "        print('there was an I/O error trying to open the data file: ' + str( iox ))\n",
    "        sys.exit()\n",
    "    return rawdata\n",
    "\n",
    "# Return a list with the possible sentiments that a tweet might have.\n",
    "def get_sentiments(df):\n",
    "    sentis = df[\"Sentiment\"].unique()\n",
    "    sentis = list(sentis)\n",
    "    return sentis\n",
    "\n",
    "# Return a string containing the second most popular sentiment among the tweets.\n",
    "def second_most_popular_sentiment(df):\n",
    "    return  df[\"Sentiment\"].value_counts().index[1]\n",
    "\n",
    "# Return the date (string as it appears in the data) with the greatest number of extremely positive tweets.\n",
    "def date_most_popular_tweets(df):\n",
    "    res = df.loc[df[\"Sentiment\"] == 'Extremely Positive']\n",
    "    date = list(res[\"TweetAt\"])\n",
    "    return date[len(date)-1]\n",
    "\n",
    "# Modify the dataframe df by converting all tweets to lower case. \n",
    "def lower_case(df):\n",
    "    df['OriginalTweet'] = df['OriginalTweet'].str.lower()\n",
    "    return df\n",
    "\n",
    "# Modify the dataframe df by replacing each characters which is not alphabetic or whitespace with a whitespace.\n",
    "def remove_non_alphabetic_chars(df):\n",
    "    df['OriginalTweet'] = df['OriginalTweet'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "\n",
    "\n",
    "# Modify the dataframe df with tweets after removing characters which are not alphabetic or whitespaces.\n",
    "def remove_multiple_consecutive_whitespaces(df):\n",
    "    df['OriginalTweet'] = df['OriginalTweet'].str.replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "# Given a dataframe where each tweet is one string with words separated by single whitespaces,\n",
    "# tokenize every tweet by converting it into a list of words (strings).\n",
    "def tokenize(df):\n",
    "    df['OriginalTweet'] = df['OriginalTweet'].apply(lambda row: [word for word in row.split()])\n",
    "    \n",
    "    \n",
    "# Given dataframe tdf with the tweets tokenized, return the number of words in all tweets including repetitions.\n",
    "def count_words_with_repetitions(tdf):\n",
    "    a = list(df[\"OriginalTweet\"])\n",
    "    a = [col for row in a for col in row ]\n",
    "    count = len(a)\n",
    "    return count\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, return the number of distinct words in all tweets.\n",
    "def count_words_without_repetitions(tdf):\n",
    "    a = list(df[\"OriginalTweet\"])\n",
    "    a = [col for row in a for col in row ]\n",
    "    a = set(a)\n",
    "    count = len(a)\n",
    "    return count\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, return a list with the k distinct words that are most frequent in the tweets.\n",
    "def frequent_words(tdf,k):\n",
    "    a = list(df[\"OriginalTweet\"])\n",
    "    a = [col for row in a for col in row ]\n",
    "    freq = list(pd.value_counts(a).head(k).index)\n",
    "    return freq\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, remove stop words and words with <=2 characters from each tweet.\n",
    "# The function should download the list of stop words via:\n",
    "# https://raw.githubusercontent.com/fozziethebeat/S-Space/master/data/english-stop-words-large.txt\n",
    "def remove_stop_words(tdf):\n",
    "    #get the stopwords\n",
    "    stopwords = requests.get(\"https://raw.githubusercontent.com/fozziethebeat/S-Space/master/data/english-stop-words-large.txt\" ).content.decode(\"UTF-8\").split(\"\\n\")\n",
    "    df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x: [word for word in x if (len(word)>2 and word not in stopword)])\n",
    "    \n",
    "# Given dataframe tdf with the tweets tokenized, reduce each word in every tweet to its stem.\n",
    "def stemming(tdf):\n",
    "    stemmer = PorterStemmer()\n",
    "    df['OriginalTweet'] = df['OriginalTweet'].apply(lambda row: [stemmer.stem(word) for word in row])\n",
    "\n",
    "# Given a pandas dataframe df with the original coronavirus_tweets.csv data set,\n",
    "# build a Multinomial Naive Bayes classifier. \n",
    "# Return predicted sentiments (e.g. 'Neutral', 'Positive') for the training set\n",
    "# as a 1d array (numpy.ndarray). \n",
    "def mnb_predict(df):\n",
    "    lower_case(df)\n",
    "    remove_non_alphabetic_chars(df)\n",
    "    remove_multiple_consecutive_whitespaces(df)\n",
    "    tokenize(df)\n",
    "    remove_stop_words(df)\n",
    "    stemming(df)\n",
    "    \n",
    "    a = list(df[\"OriginalTweet\"])\n",
    "    words = [col for row in a for col in row ]\n",
    "    \n",
    "    cv = CountVectorizer(min_df=5)\n",
    "    cv_fit = cv.fit_transform(words)\n",
    "    # cv.vocabulary_\n",
    "    terms = list(cv.vocabulary_)\n",
    "    termdoc = [[0 for j in range(len(terms))]  for i in range(len(df))]\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(terms)):\n",
    "            if(terms[j] in  df[\"OriginalTweet\"][i]):\n",
    "                termdoc[i][j] = cv.vocabulary_[terms[j]] \n",
    "    label = list(df[\"Sentiment\"])\n",
    "    clf = nb.MultinomialNB()\n",
    "    clf.fit(termdoc,label)\n",
    "    \n",
    "    y_hat = clf.predict(termdoc)\n",
    "    return y_hat\n",
    "# Given a 1d array (numpy.ndarray) y_pred with predicted labels (e.g. 'Neutral', 'Positive') \n",
    "# by a classifier and another 1d array y_true with the true labels, \n",
    "# return the classification accuracy rounded in the 3rd decimal digit.\n",
    "def mnb_accuracy(y_pred,y_true):\n",
    "    count = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if (y_true[i] == y_pred[i]):\n",
    "            count += 1\n",
    "    acc = count / len(y_pred)\n",
    "    acc = round(acc,3)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "bd69edb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv_3(\"coronavirus_tweets.csv\")\n",
    "y_true = np.array(df[\"Sentiment\"])\n",
    "y_pred = mnb_predict(df)\n",
    "mnb_accuracy(y_pred,y_true)\n",
    "#3 1500 0.47\n",
    "#5 1500 0.474\n",
    "#10 1500 0.473\n",
    "#5 3000 0.51\n",
    "#5 2000 0.487\n",
    "#5 max 0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bcea5b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = read_csv_3(\"coronavirus_tweets.csv\")\n",
    "sen = get_sentiments(df)\n",
    "\n",
    "sec = second_most_popular_sentiment(df)\n",
    "\n",
    "\n",
    "data_most = date_most_popular_tweets(df)\n",
    "\n",
    "\n",
    "# lower_case(df)\n",
    "# # print(df)\n",
    "# remove_non_alphabetic_chars(df)\n",
    "# remove_multiple_consecutive_whitespaces(df)\n",
    "\n",
    "# tokenize(df)\n",
    "mnb_predict(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3518bcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>[menyrbi, phil, gahan, chrisitv, http, ifz, fa...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>[advic, talk, neighbour, famili, exchang, phon...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>[coronaviru, australia, woolworth, give, elder...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>[food, stock, empti, don, panic, food, stay, c...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>[readi, supermarket, covid, outbreak, paranoid...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>[airlin, pilot, offer, stock, supermarket, she...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>44952</td>\n",
       "      <td>89904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>[respons, complaint, provid, cite, covid, rela...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>44953</td>\n",
       "      <td>89905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>[tough, kameronwild, ration, toilet, paper, co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>44954</td>\n",
       "      <td>89906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>[wrong, smell, hand, sanit, start, turn, coron...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>[tartiicat, rift, amazon, normal, market, pric...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location     TweetAt  \\\n",
       "0          3799       48751                        London  16-03-2020   \n",
       "1          3800       48752                            UK  16-03-2020   \n",
       "2          3801       48753                     Vagabonds  16-03-2020   \n",
       "3          3802       48754                           NaN  16-03-2020   \n",
       "4          3803       48755                           NaN  16-03-2020   \n",
       "...         ...         ...                           ...         ...   \n",
       "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
       "41153     44952       89904                           NaN  14-04-2020   \n",
       "41154     44953       89905                           NaN  14-04-2020   \n",
       "41155     44954       89906                           NaN  14-04-2020   \n",
       "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \n",
       "0      [menyrbi, phil, gahan, chrisitv, http, ifz, fa...             Neutral  \n",
       "1      [advic, talk, neighbour, famili, exchang, phon...            Positive  \n",
       "2      [coronaviru, australia, woolworth, give, elder...            Positive  \n",
       "3      [food, stock, empti, don, panic, food, stay, c...            Positive  \n",
       "4      [readi, supermarket, covid, outbreak, paranoid...  Extremely Negative  \n",
       "...                                                  ...                 ...  \n",
       "41152  [airlin, pilot, offer, stock, supermarket, she...             Neutral  \n",
       "41153  [respons, complaint, provid, cite, covid, rela...  Extremely Negative  \n",
       "41154  [tough, kameronwild, ration, toilet, paper, co...            Positive  \n",
       "41155  [wrong, smell, hand, sanit, start, turn, coron...             Neutral  \n",
       "41156  [tartiicat, rift, amazon, normal, market, pric...            Negative  \n",
       "\n",
       "[41157 rows x 6 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "61e1d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(df[\"OriginalTweet\"])\n",
    "words = [col for row in a for col in row ]\n",
    "\n",
    "cv = CountVectorizer(min_df=5,max_features=500)\n",
    "cv_fit = cv.fit_transform(words)\n",
    "# cv.vocabulary_\n",
    "termdoc = [[0 for j in range(len(cv.vocabulary_))]  for i in range(len(df))]\n",
    "terms = list(cv.vocabulary_)\n",
    "\n",
    "# list(cv_fit)\n",
    "# cv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1f53a9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41157"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    for j in range(len(terms)):\n",
    "        if(terms[j] in  df[\"OriginalTweet\"][i]):\n",
    "            termdoc[i][j] = cv.vocabulary_[terms[j]] \n",
    "len(termdoc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "88053b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = list(df[\"Sentiment\"])\n",
    "clf = nb.MultinomialNB()\n",
    "clf.fit(termdoc,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e8bd31b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4057875938479481"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = clf.predict(termdoc)\n",
    "type(label)\n",
    "count = 0\n",
    "for i in range(len(label)):\n",
    "    if (label[i] == y_hat[i]):\n",
    "        count += 1\n",
    "count / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7497a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['menyrbi', 'phil', 'gahan', 'chrisitv', 'http', 'ifz', 'fan', 'http', 'ghgfzcc', 'http', 'nlzdxno', 'advic', 'talk', 'neighbour', 'famili', 'exchang', 'phone', 'number', 'creat', 'contact', 'list', 'phone', 'number', 'neighbour', 'school', 'employ', 'chemist', 'set', 'onlin', 'shop', 'account', 'poss', 'adequ', 'suppli', 'regular', 'med', 'order']\n",
      "['menyrbi' 'phil' 'gahan' 'chrisitv' 'http' 'ifz' 'fan' 'http' 'ghgfzcc'\n",
      " 'http' 'nlzdxno' 'advic' 'talk' 'neighbour' 'famili' 'exchang' 'phone'\n",
      " 'number' 'creat' 'contact' 'list' 'phone' 'number' 'neighbour' 'school'\n",
      " 'employ' 'chemist' 'set' 'onlin' 'shop' 'account' 'poss' 'adequ' 'suppli'\n",
      " 'regular' 'med' 'order']\n",
      "<class 'numpy.ndarray'>\n",
      "dic is: {'http': 0, 'neighbour': 1, 'number': 2}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13064/3645978366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcv_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dic is: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dic is: \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "text = df['OriginalTweet'][0]+df[\"OriginalTweet\"][1]\n",
    "\n",
    "print(text)\n",
    "text = np.array(text)\n",
    "print(text)\n",
    "print(type(text))\n",
    "cv = CountVectorizer(min_df= 2,max_features=3)\n",
    "cv_fit = cv.fit_transform(text)\n",
    "print(\"dic is: \"+str(cv.vocabulary_))\n",
    "print(\"dic is: \"+ str(len(cv.vocabulary_)))\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a46db5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "a = ['a','b','c']\n",
    "b = ['a',2,3]\n",
    "b = np.array(b)\n",
    "for i in range(3):\n",
    "    if(a[i] == b[i]):\n",
    "        print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "318826b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.234"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3.234487293\n",
    "round(a,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
